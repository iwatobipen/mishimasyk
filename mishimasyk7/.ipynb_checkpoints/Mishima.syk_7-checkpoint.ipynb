{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MedChem meets Deep Learning\n",
    "\n",
    "- 2015/12/12\n",
    "- mishima.syk \\#7\n",
    "- @iwatobipen\n",
    "\n",
    "\n",
    "<img src='penguin.jpg' width=200 height=200 align='right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "本当はSphinxで作りたかったんだけど。。。\n",
    "===================\n",
    "\n",
    "- Python3@anaconda でs6 のインストールでこけたので\n",
    "- iPython からスラド作ってみました。\n",
    "- ちなみに知ってるかもしれないけどコンバートは下記のコマンド\n",
    "\n",
    "```\n",
    "    iwatobipen$ ipython nbconvert yournote.ipynb --to slides --post serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "自己紹介\n",
    "=====\n",
    "\n",
    "- 某製薬企業のMedicinal Chemist\n",
    "- Skill Python, JavaScript, R　などを少々\n",
    "- ランニング好き（遅いケド）\n",
    "- 今年某ソフトのユーザー会の会長になってしまいました。:(；ﾞﾟ'ωﾟ'):\n",
    "- たまにつぶやいたり blogを書いたりします。\n",
    "https://iwatobipen.wordpress.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "今日はDLを使ってみました的な内容です。\n",
    "=================\n",
    "\n",
    "- Tools ( Scikit learn, Chainer, TensorFlow!? )\n",
    "- さらに時間が余ればメドケムに使えそうな話題をちょっと・・・"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PythonでDL\n",
    "======\n",
    "\n",
    "* 最近のトレンドでは\n",
    " - Chainer\n",
    " - TensorFlow\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://chainer.org/\" width=600 height=400></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML( '<iframe src=\"http://chainer.org/\" width=600 height=400></iframe>' )\n",
    "#一つはchainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.tensorflow.org/\" width=600 height=400></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML( '<iframe src=\"https://www.tensorflow.org/\" width=600 height=400></iframe>' )\n",
    "#もう一つはTensorflowが最近話題ですね！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "今回やった手順\n",
    "========\n",
    "\n",
    "* CHEMBL DBからhERG阻害に関するデータを取得した。\n",
    "* RDKitで懲りずにECFP( 構造を01のビット列に変形する )を計算\n",
    "    - きっとこの辺が一番議論すべき点ですね、、、本来 \n",
    "* AMES posi/negaの分類モデルをchainer, tensorflowで実装してみた。\n",
    "* ねたもと\n",
    "    - herg me out !http://pubs.acs.org/doi/abs/10.1021/ci400308z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DataPreparation\n",
    "==========\n",
    "\n",
    "大体データの準備は下記のような雰囲気にした。\n",
    "Chainer => 0, 1でポジ、ネガのフラグとしたがTensorflowはうまくいかないので配列としておいた。[0,1][1,0]\n",
    "\n",
    "```python\n",
    "from __future__ import print_function\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# fileをdumpして保存したいので。\n",
    "import cPickle\n",
    "\n",
    "df = pd.read_table('bioactivity-15_13-09-28.txt', header=0)\n",
    "df_bind = df[ df.ASSAY_TYPE==\"B\" ]\n",
    "df_bind = df_bind[ df_bind.STANDARD_VALUE != None ]\n",
    "rows = df_bind.shape[ 0 ]\n",
    "mols = [ ]\n",
    "act = [ ]\n",
    "fps = []\n",
    "def act2bin( val ):\n",
    "    if val > 10000:\n",
    "        return 0  \n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "for i in range( rows ):\n",
    "    try:\n",
    "        smi = df_bind.CANONICAL_SMILES[i]\n",
    "        mol = Chem.MolFromSmiles( smi )\n",
    "        if mol != None:\n",
    "            mols.append( mol )\n",
    "            act.append( act2bin( df_bind.STANDARD_VALUE[i]) )\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "for mol in mols:\n",
    "    arr = np.zeros( (1,) )\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect( mol, 2 )\n",
    "    DataStructs.ConvertToNumpyArray( fp, arr )\n",
    "    fps.append( fp )\n",
    "\n",
    "fps = np.array( fps, dtype = np.float32 )\n",
    "act = np.array( act, dtype = np.int32 )\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split( fps, act, test_size=0.3, random_state=455 )\n",
    "\n",
    "f = open('dataset.pkl', 'wb')\n",
    "cPickle.dump( [train_x, train_y, test_x, test_y ], f )\n",
    "f.close()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chainerでの実装\n",
    "=========\n",
    "\n",
    "* Pros\n",
    "   - easy to install ( pip )\n",
    "   - easy to use GPU ( pycudaなど諸々の設定は必要 )\n",
    "* Cons\n",
    "   - GPUを使う場合はコードが少し変わる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "ChainerでGPUを使った場合のサンプル\n",
    "=================\n",
    "\n",
    "```python\n",
    "#data = x_train, y_train, x_test, y_test\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from chainer import optimizers, cuda, Function, FunctionSet, gradient_check, Variable, utils\n",
    "from chainer import optimizer\n",
    "import chainer.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use( 'ggplot')\n",
    "\n",
    "#データセットの読み込み\n",
    "inf = open('dataset.pkl', 'rb')\n",
    "dataset = cPickle.load( inf )\n",
    "train_x, train_y, test_x, test_y = dataset\n",
    "\n",
    "#モデルの設定＝＞最後に　GPUに受け渡している　to.gpuの部分\n",
    "#Linear関数にて各層のノード数を設定する。今回は2048ビットの入力でポジ、ネガの２値の出力となる。\n",
    "model1 = FunctionSet(\n",
    "        l1 = F.Linear( 2048, 1500 ),\n",
    "        l2 = F.Linear( 1500, 500 ),\n",
    "        l3 = F.Linear( 500, 100 ),\n",
    "        l4 = F.Linear( 100, 100 ),\n",
    "        l5 = F.Linear( 100, 20 ),\n",
    "        l6 = F.Linear( 20, 2 )\n",
    "        ).to_gpu()\n",
    "#最適化の設定＝＞最急下降方ではなくAdaGradを使った。\n",
    "optimizer = optimizers.AdaGrad()\n",
    "optimizer.setup( model1 )\n",
    "\n",
    "# forward関数の定義\n",
    "# 誤差の計算はソフトマックス交差エントロピーとしておく。\n",
    "# x,y の入れ物はVariableで定義\n",
    "def forward( x_data, y_data ):\n",
    "    x = Variable( x_data )\n",
    "    t = Variable( y_data )\n",
    "    # Drop outの実装もシンプル\n",
    "    #h1 = F.dropout( F.relu( model1.l1( x ) ), train=train)\n",
    "    h1 = F.relu( model1.l1( x ) )\n",
    "    h2 = F.relu( model1.l2( h1 ))\n",
    "    h3 = F.relu( model1.l3( h2 ))\n",
    "    h4 = F.relu( model1.l4( h3 ))\n",
    "    h5 = F.relu( model1.l5( h4 ))\n",
    "    y = model1.l6( h5 )\n",
    "    return F.softmax_cross_entropy( y, t ), F.accuracy( y, t )\n",
    "\n",
    "batchsize = 100\n",
    "datasize = len( train_y )\n",
    "print( datasize )\n",
    "\n",
    "counter = 0\n",
    "log_f = open( 'train_log.txt', 'w' )\n",
    "log_f.write(\"counter, epoch, i, loss, accuracy\\n\")\n",
    "\n",
    "train_loss= []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc =[]\n",
    "\n",
    "#50世代学習させる。\n",
    "for epoch in range( 50 ):\n",
    "    print( \"epoch %d\"%epoch )\n",
    "    indexes = np.random.permutation( datasize )\n",
    "    for i in range(0, datasize, batchsize ):\n",
    "        counter += 1\n",
    "        # GPUに受け渡すのはcuda.to_gpuで良い。GPU使わない場合はこのおまじないを消す。\n",
    "        xbatch = cuda.to_gpu( train_x[ indexes[i : i+batchsize ]])\n",
    "        ybatch = cuda.to_gpu( train_y[ indexes[i : i+batchsize ]])\n",
    "        optimizer.zero_grads()\n",
    "        loss, accuracy = forward( xbatch, ybatch )\n",
    "        #バックプロパゲーション＋ログの出力などなど\n",
    "        loss.backward()\n",
    "        log_f.write( \"%s,%s,%s,%s,%s\\n\"%( counter, epoch, i, loss.data, accuracy.data))\n",
    "        optimizer.update()\n",
    "        train_loss.append( loss.data )\n",
    "        train_acc.append( accuracy.data )\n",
    "    # トレーニングセットで予測してみる。\n",
    "    for i in range(0, len(test_x), batchsize ):\n",
    "        x_batch = cuda.to_gpu( test_x[i : i+batchsize] )\n",
    "        y_batch = cuda.to_gpu( test_y[ i : i+batchsize] )\n",
    "        loss, accuracy = forward(x_batch, y_batch )\n",
    "        test_loss.append( loss.data )\n",
    "        test_acc.append( accuracy.data )\n",
    "        print( loss.data, accuracy.data )\n",
    "\n",
    "log_f.close()\n",
    "\n",
    "plt.Figure( figsize=( 8,6 ) )\n",
    "plt.plot( range(len(train_loss)), train_loss )\n",
    "plt.plot( range(len(train_acc)), train_acc)\n",
    "plt.legend( [\"train_loss\", \"train_acc\"], loc=4 )\n",
    "plt.savefig('train.png')\n",
    "plt.close()\n",
    "\n",
    "plt.Figure( figsize=( 8,6 ) )\n",
    "plt.plot( range(len(test_loss)), test_loss )\n",
    "plt.plot( range(len(test_acc)), test_acc)\n",
    "plt.legend( [\"test_loss\", \"test_acc\"], loc=4 )\n",
    "plt.savefig('test.png')\n",
    "plt.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "上記のコード実行の結果を見てみよう\n",
    "================\n",
    "\n",
    "- トレーニングはいい感じ\n",
    "<img src='train.png' width=300 height=200 >\n",
    "\n",
    "- テストは世代を追っても全然精度が上がっていない\n",
    "<img src='test.png' width=300 height=200 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "多分実装の仕方が間違っている\n",
    "=============\n",
    "\n",
    "<img src='gakkari.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "ちなみに同様のコードをGPUあるなしで実行させてみると\n",
    "========================\n",
    "\n",
    "GPU使っているからCPU user時間がだいぶ違う\n",
    "GPU: NVIDIA GeForce GT 750M\n",
    "\n",
    " |GPU|なし|あり|\n",
    " |:--:|:--:|:--:|\n",
    " |real|2m50|1m22|\n",
    " |user|5m12|*1m20|\n",
    " |sys |42 | 1.5|\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "気を取り直してTensorFlowでの実装\n",
    "================\n",
    "\n",
    "* Pros\n",
    "   - easy to install ( pip )\n",
    "   - easy to use GPU ( OSxでは未対応らしい )\n",
    "   - tensorboardがイケてるらしい\n",
    "* Cons\n",
    "   - python3.x未対応\n",
    "   - コードのわかりやすさ　Theano < Tensorflow < Chainer かな？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tensorflowを使った場合のサンプル\n",
    "=================\n",
    "\n",
    "- チュートリアルのMNISTのサンプルコードほぼコピペ\n",
    "- GPU未対応（OSX）\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle\n",
    "\n",
    "# データの読み込み\n",
    "f = open('dataset_for_tf.pkl', 'rb')\n",
    "train_x, train_y , test_x, test_y = cPickle.load( f )\n",
    "nbit = len( train_x[0] )\n",
    "# Placeholderという説明変数、目的変数の受け皿を準備\n",
    "# xは行列になるので最終的には N x 2048の入力ってことになる。\n",
    "x = tf.placeholder( \"float\", [ None, 2048 ] )\n",
    "y = tf.placeholder( \"float\", [ None, 2 ] )\n",
    "\n",
    "# 隠れ層、出力層の重み、バイアス項の定義\n",
    "# y = Wx + b という線形結合で表現する\n",
    "# ちょっとここが面倒な気がした。\n",
    "w_h = tf.Variable( tf.random_normal( [ 2048, 100 ], mean=0.0, stddev=0.05 ))\n",
    "w_o = tf.Variable( tf.random_normal( [ 100, 2 ], mean=0.0, stddev=0.05 ))\n",
    "b_h = tf.Variable( tf.zeros( [100] ))\n",
    "b_o = tf.Variable( tf.zeros( [2] ))\n",
    "\n",
    "# 正則化項\n",
    "L2_sqr = tf.nn.l2_loss( w_h ) + tf.nn.l2_loss( w_o )\n",
    "lambda_2 = 0.01\n",
    "\n",
    "# モデルの定義　先のコードで定義しているからその辺マルッと使えばいい\n",
    "# 活性化関数はreluを使ったけどtanhとか適したものを使えばいい\n",
    "def model( X, w_h, b_h, w_o, b_o ):\n",
    "    h = tf.nn.relu( tf.matmul( X, w_h ) + b_h )\n",
    "    pyx = tf.nn.softmax( tf.matmul( h, w_o ) + b_o )\n",
    "    return pyx\n",
    "# 誤差関数の定義　w係数でL２正則化を科す\n",
    "# tensorboard使いたいから交差エントロピーを記録しておく\n",
    "def loss( output, supervisor_labels_placeholder ):\n",
    "    cross_entropy = -tf.reduce_sum( supervisor_labels_placeholder * tf.log(output))\n",
    "    # 誤差を記録しておくコード↓\n",
    "    tf.scalar_summary(\"entropy\", cross_entropy )\n",
    "    return cross_entropy + lambda_2 * L2_sqr\n",
    "    #return cross_entropy\n",
    "\n",
    "# トレーニング＆最適化＝＞ここでバックプロパゲーションやってるはず。\n",
    "def training( loss ):\n",
    "    train_step = tf.train.GradientDescentOptimizer( 0.01 ).minimize( loss )\n",
    "    return train_step\n",
    "\n",
    "num_x = len(train_x)\n",
    "batch = num_x / 500\n",
    "\n",
    "# merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    total_step = 0\n",
    "    #予測の実施\n",
    "    y_hypo = model( x, w_h, b_h, w_o, b_o )\n",
    "    correct_prediction = tf.equal( tf.argmax( y_hypo, 1), tf.argmax( y, 1 ))\n",
    "    accuracy = tf.reduce_mean( tf.cast( correct_prediction, 'float' ))\n",
    "    # 精度のログ取り\n",
    "    tf.scalar_summary(\"acc\", accuracy )\n",
    "    loss = loss( y_hypo, y )\n",
    "    training_op = training( loss )\n",
    "    # ログのまとめ\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    # ./testlogってホルダにログを入れる＝＞後でtensorboardでみる。\n",
    "    summary_writer = tf.train.SummaryWriter('./testlog', sess.graph_def)\n",
    "    print('Training!')\n",
    "    for i in range( 100 ):\n",
    "        for j in range(batch):\n",
    "            total_step += 1\n",
    "            batch_xs, batch_ys = train_x[j:j+batch], train_y[j:j+batch]\n",
    "            sess.run( training_op,  { x: batch_xs, y: batch_ys })\n",
    "            #sess.run( training_op )\n",
    "        if i%5 == 0:\n",
    "            train_accuracy = accuracy.eval({x:batch_xs, y:batch_ys} )\n",
    "            #tf.scalar_summary(\"acc\", train_accuracy )\n",
    "            print('batch%s total_%s,%s'%(j,total_step, train_accuracy))\n",
    "            print( sess.run( loss, {x: batch_xs, y :batch_ys } ))\n",
    "            summary_str = sess.run( merged_summary_op, {x: batch_xs, y :batch_ys }  )\n",
    "            summary_writer.add_summary( summary_str, total_step )\n",
    "\n",
    "            test_acc = accuracy.eval( {x : test_x, y : test_y })\n",
    "            print(\"TEST\")\n",
    "            print(test_acc)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:6007\" width=800 height=600></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML( '<iframe src=\"http://localhost:6007\" width=800 height=600></iframe>' )\n",
    "#起動はこんなコマンドで。\n",
    "#iwatobipen$ tensorboard --logdir={absolute Path} --port 6007\n",
    "#グラフ構造が可視化できるのは面白いと思いますね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chainer と Tensorflowでコードを書いてみたんですが\n",
    "=========================\n",
    "\n",
    "- モデルの定義などがChainerの方が少し書きやすいように思った\n",
    "- GPUの使用もそんなに苦にならない（シングルGPUのケースしか見ていませんが）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ちなみにScikit-Learnで実装すると\n",
    "=================\n",
    "\n",
    "- 何を使う場合も大体要領は同じ\n",
    "- fit関数でモデルを構築、predictで予測実施。\n",
    "- 扱いやすい♩\n",
    "\n",
    "```python\n",
    "import cPickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "inf = open(\"dataset.pkl\", \"rb\")\n",
    "train_x, train_y, test_x, test_y = cPickle.load( inf )\n",
    "\n",
    "rfcls = RandomForestClassifier( n_estimators = 10 )\n",
    "rfcls = rfcls.fit( train_x, train_y )\n",
    "rpred = rfcls.predict( test_x )\n",
    "print( \"RF\" )\n",
    "print( metrics.accuracy_score( test_y,rpred ))\n",
    "print( metrics.confusion_matrix( test_y,rpred ))\n",
    "\n",
    "\n",
    "svcls = SVC()\n",
    "svcls.fit( train_x, train_y )\n",
    "svcpred = svcls.predict( test_x )\n",
    "print(\"SVC\")\n",
    "print( metrics.accuracy_score( test_y, svcpred ))\n",
    "print( metrics.confusion_matrix( test_y, svcpred ))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Confution Matrixは\n",
    "==========\n",
    "\n",
    " RF\n",
    " 0.793609671848\n",
    "\n",
    " |  |PP |PN  |\n",
    " |--|--|-----|\n",
    " |TP|548|240 |\n",
    " |TN|238|1290|\n",
    "\n",
    "\n",
    "\n",
    " SVC\n",
    " 0.673143350604\n",
    "\n",
    " |  |PP |PN  |\n",
    " |--|---|----|\n",
    " |TP|35 |753 |\n",
    " |TN|4  |1524|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "論文報告の結果はこちら\n",
    "==========\n",
    "\n",
    "- random forest優秀ですね。\n",
    "\n",
    "\n",
    "<img src='ref.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "結局もやもやしてる\n",
    "=========\n",
    "\n",
    "- 大量のデータを与えて特徴を抽出するというプロセスがキーだとして\n",
    "- FingerPrint, Descriptorが入力情報として適当なのか？\n",
    "- データが少ないのか？\n",
    "- 入力に対してPharmacophoreのような特徴量が抽出できる入力ってなんだろう。\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
